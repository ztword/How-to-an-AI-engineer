

# 核心运算，点积（内积）：从数学概念到RAG检索的底层密码

## 开篇，为什么你必须搞懂点积

很多人第一次听到点积这个词，可能是在高中数学课上。老师在黑板上写了一个公式，你背下来应付了考试，然后就忘了。说实话，我当年也是这样。直到后来做NLP相关的项目，搞embedding检索的时候，我才真正意识到，这个看起来平平无奇的运算，居然是整个现代语义搜索系统的数学地基。

今天大模型和RAG系统这么火，每个人都在聊向量数据库、聊语义检索、聊embedding。但你有没有想过一个问题，当你在一个知识库里输入一个问题，系统是怎么在几百万条文本里瞬间找到最相关的那几条的？答案就藏在点积里。

所以这篇文章，我想从最基础的概念开始，一步步带你把点积这件事彻底搞明白。我们不光要知道它是什么，还要知道它为什么重要，以及它是怎么撑起整个RAG检索系统的。

---

## 第一部分，点积的定义，其实就是对应相乘再求和

我们先从最基本的定义说起。

假设你有两个向量，a和b。向量说白了就是一组有顺序的数字。比如a是[1, 2, 3]，b是[4, 5, 6]。点积的计算方式极其简单，把对应位置的数字相乘，然后把所有乘积加起来。

```
a = [1, 2, 3]
b = [4, 5, 6]

点积 = 1×4 + 2×5 + 3×6 = 4 + 10 + 18 = 32
```

就这么简单。没有什么矩阵变换，没有什么高深操作，就是乘一乘加一加。

用数学符号写出来就是，a·b = Σ(ai × bi)，其中i从1到n，n是向量的维度。

你可能觉得，这也太简单了吧，有什么好讲的。但恰恰是这种简单，让它在计算机里跑起来极其高效。GPU天生擅长做大量的乘法和加法，而点积刚好就是这两种运算的组合。这也是为什么当embedding维度高达768甚至1536的时候，计算机仍然能在毫秒级别完成海量的点积运算。

我个人的理解是，好的数学工具往往都有一个共同特点，定义上足够简单，含义上足够深刻。点积就是这样的典型。

说到这里，我们需要把维度拉高一点来感受一下实际场景。在NLP领域，一个句子经过embedding模型处理后，通常会变成一个768维或者1536维的向量。也就是说，这个向量里有768个或者1536个数字。两个句子之间的点积，就是这768个数字分别对应相乘再求和。维度虽然高了，但运算逻辑完全一样。

下面用Python代码展示一下，让你有个更直观的感觉。

```python
import numpy as np

# 手动计算点积
def dot_product_manual(a, b):
    result = 0
    for i in range(len(a)):
        result += a[i] * b[i]
    return result

# 用numpy计算点积
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

# 两种方式结果一样
print(dot_product_manual(a, b))  # 32
print(np.dot(a, b))              # 32

# 模拟高维embedding的点积
embedding_a = np.random.randn(768)
embedding_b = np.random.randn(768)
print(np.dot(embedding_a, embedding_b))  # 某个实数值
```

看到没有，不管是3维还是768维，点积的逻辑没有任何变化。这种一致性让它成为一个极其通用的工具。

---

## 第二部分，点积的几何意义，投影的故事

如果点积只是乘一乘加一加，那它确实不值得单独拿出来讲。真正让点积变得有灵魂的，是它的几何意义。

我们换个角度来看。在二维或三维空间里，两个向量可以画出来，它们之间有一个夹角θ。点积有一个非常优美的几何公式，

**a·b = |a| × |b| × cos(θ)**

其中|a|和|b|分别是两个向量的长度（也叫模），θ是它们之间的夹角。

这个公式告诉我们什么呢？点积本质上在度量一个东西，一个向量在另一个向量方向上的投影长度，再乘以另一个向量的长度。

举个特别直观的例子。想象你站在太阳底下，地上有你的影子。你的身体就是一个向量，地面方向就是另一个向量，你的影子长度就是投影。当你直直地站着（和地面垂直），影子最短接近于零。当你躺下来（和地面平行），影子最长，等于你的身高。

点积做的事情就像在算影子。两个向量方向越一致，投影越长，点积越大。两个向量方向垂直，投影为零，点积也为零。两个向量方向相反，投影为负，点积也是负数。

这里给大家画一个概念的对照关系，

```
夹角θ = 0°    → cos(θ) = 1   → 方向完全相同 → 点积最大（正值）
夹角θ = 90°   → cos(θ) = 0   → 方向完全垂直 → 点积为零
夹角θ = 180°  → cos(θ) = -1  → 方向完全相反 → 点积最小（负值）
```

看到这个对照关系，你有没有嗅到一点味道？方向相同意味着相似，方向垂直意味着无关，方向相反意味着对立。这不就是衡量相似度的天然工具吗？

这个观察非常关键，它是我们后面理解余弦相似度和RAG检索的基石。

---

## 第三部分，从点积到余弦相似度

现在我们已经知道点积能反映两个向量的方向关系，但它有一个小问题，点积的值会受到向量长度的影响。

比如向量a是[1, 0]，向量b是[100, 0]，它们方向完全一样，点积是100。向量c是[1, 0]，向量d是[1, 0]，它们方向也完全一样，点积却只有1。方向明明都相同，但点积值差了100倍。

这在实际应用中会带来麻烦。我们比较两个句子的语义相似度时，关心的是方向（语义是否一致），不太关心长度。两个句子说的是同一件事，一个用了10个字，一个用了100个字，它们的embedding长度可能不同，但语义应该是相似的。

怎么解决？消除长度的影响就好了。把点积的几何公式变一下形，

**cos(θ) = (a·b) / (|a| × |b|)**

这就是大名鼎鼎的余弦相似度公式。把点积除以两个向量长度的乘积，得到的就是夹角的余弦值。这个值的范围在-1到1之间，1表示完全相同，0表示完全无关，-1表示完全相反。

说白了，余弦相似度就是归一化之后的点积。它剥离了向量长度的干扰，只保留了方向信息。

实际上，现在主流的embedding模型（比如OpenAI的text-embedding-ada-002，或者开源的bge系列模型），输出的向量通常已经做过L2归一化了，也就是说向量的长度已经被统一成了1。在这种情况下，|a|和|b|都等于1，余弦相似度就直接等于点积。这就是为什么很多向量数据库默认用点积作为相似度度量方式，因为在归一化的前提下，它和余弦相似度完全等价，而且计算更快。

```python
import numpy as np

def cosine_similarity(a, b):
    dot = np.dot(a, b)
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)
    return dot / (norm_a * norm_b)

# 示例
a = np.array([1, 2, 3])
b = np.array([2, 4, 6])  # 方向和a相同，只是长度不同
c = np.array([3, -1, 0]) # 和a方向差异较大

print(f"a和b的余弦相似度，{cosine_similarity(a, b):.4f}")  # 1.0000
print(f"a和c的余弦相似度，{cosine_similarity(a, c):.4f}")  # 接近0

# 归一化后，点积等于余弦相似度
a_norm = a / np.linalg.norm(a)
b_norm = b / np.linalg.norm(b)
print(f"归一化后的点积，{np.dot(a_norm, b_norm):.4f}")  # 1.0000
```

这段代码清楚地展示了，归一化之后点积和余弦相似度的等价关系。

---

## 第四部分，为什么点积是RAG系统检索的数学基础

好了，前面铺垫了这么多，现在终于可以聊到最核心的应用场景了。

RAG，也就是检索增强生成（Retrieval-Augmented Generation），是当前大模型应用中最主流的架构之一。它的基本流程是这样的，

```
用户提问
   ↓
问题经过embedding模型转成向量（query向量）
   ↓
在向量数据库中，用query向量和所有文档向量做点积/余弦相似度计算
   ↓
取出相似度最高的Top-K个文档片段
   ↓
把这些文档片段和原始问题一起送给大模型
   ↓
大模型基于检索到的上下文生成回答
```

你看到了吗？整个检索过程的核心操作就是点积。向量数据库里可能存着几百万个文档片段的embedding，每一次用户提问，系统都需要把这个问题的embedding和所有文档embedding做点积运算，然后按照分数从高到低排序，挑出最相关的几条。

这里有一个我特别想强调的观点，RAG系统的检索质量，本质上取决于两件事。第一是embedding模型的质量，它决定了语义信息能否被准确地编码到向量中。第二是相似度度量方式的选择，而目前几乎所有主流方案都选了余弦相似度，也就是归一化后的点积。

换句话说，如果embedding模型把两个语义相近的句子映射到了方向相近的向量上，那么点积就能捕捉到这种相近关系。如果embedding模型没能做到这一点，那再精妙的检索算法也无济于事。

来看一个完整的代码示例，模拟RAG中的检索过程，

```python
import numpy as np

# 模拟文档库的embedding（实际中由embedding模型生成）
documents = [
    "今天天气真好适合出去散步",
    "机器学习中梯度下降是最常用的优化方法",
    "Python是数据科学领域最流行的编程语言",
    "RAG系统通过检索外部知识来增强大模型的回答",
    "向量数据库存储高维向量并支持快速近似搜索",
    "春天来了花儿都开了",
]

# 为了演示，用随机向量模拟embedding
# 实际中应该用embedding模型（如OpenAI、BGE等）生成
np.random.seed(42)
doc_embeddings = np.random.randn(len(documents), 128)

# L2归一化
doc_embeddings = doc_embeddings / np.linalg.norm(doc_embeddings, axis=1, keepdims=True)

# 模拟用户提问的embedding
query = "向量检索是怎么工作的"
query_embedding = np.random.randn(128)
query_embedding = query_embedding / np.linalg.norm(query_embedding)

# 计算query和所有文档的点积（归一化后等于余弦相似度）
similarities = np.dot(doc_embeddings, query_embedding)

# 按相似度排序，取Top-3
top_k = 3
top_indices = np.argsort(similarities)[::-1][:top_k]

print("检索结果（按相似度降序）")
print("-" * 50)
for idx in top_indices:
    print(f"相似度，{similarities[idx]:.4f} | 文档，{documents[idx]}")
```

虽然这里用的是随机向量，但整个检索逻辑和真实的RAG系统完全一致。在真实系统中，只需要把随机向量替换成embedding模型的输出，整套流程就能跑起来。

---

## 第五部分，衡量文本embedding相似度的实战思考

聊到这里，你可能会问，余弦相似度到底多大算相似？0.8算不算高？0.5算不算低？

这个问题没有标准答案，它取决于你用的embedding模型和具体任务。但我可以分享一些经验性的参考，

```
┌──────────────────┬────────────────────────────────┐
│  余弦相似度范围    │  通常含义                       │
├──────────────────┼────────────────────────────────┤
│  0.85 - 1.00     │  高度相似，几乎在说同一件事      │
│  0.70 - 0.85     │  比较相似，话题相关且内容接近     │
│  0.50 - 0.70     │  有一定关联，但内容差异明显       │
│  0.30 - 0.50     │  关联较弱，可能只是个别词语重叠   │
│  0.00 - 0.30     │  基本无关                       │
└──────────────────┴────────────────────────────────┘
```

但我要特别提醒一点，这个表格只是粗略的经验参考。不同的embedding模型，其相似度分布差异非常大。有些模型的输出天然就比较集中，两个不太相关的文本相似度可能也有0.6。有些模型的区分度比较好，不相关文本的相似度可能只有0.2。所以在实际项目中，你最好拿自己的数据做一些测试，摸清你使用的模型的相似度分布特征，再据此设定检索阈值。

另外还有一点很重要。余弦相似度衡量的是语义方向上的相似性，但语义这个东西本身就是模糊的。比如下面两个句子，

第一句，我喜欢吃苹果。第二句，苹果公司发布了新产品。

这两句话都包含苹果这个词，但语义完全不同。一个好的embedding模型应该能区分这种多义词的情况，让这两个句子的embedding余弦相似度比较低。但如果模型能力不足，可能会给出一个偏高的相似度，这就会导致RAG检索时召回不相关的文档。

这也是为什么我一直强调，点积和余弦相似度只是工具，工具本身没有好坏之分，关键在于给它输入什么样的数据。embedding的质量才是整个系统的天花板。

---

## 第六部分，用一张思维导图串起所有知识点

到这里，我们已经从点积的基础定义讲到了它在RAG系统中的实际应用。让我用一张思维导图把整个知识脉络串起来，

```
                        点积（内积）
                            │
            ┌───────────────┼───────────────┐
            │               │               │
         基础定义          几何意义        核心应用
            │               │               │
    对应分量相乘求和    投影长度的度量     余弦相似度
            │               │               │
     a·b = Σ(ai×bi)   a·b = |a||b|cosθ   cos(θ) = a·b/(|a||b|)
            │               │               │
     计算简单高效       方向关系的表达     归一化后等价于点积
            │               │               │
            └───────┬───────┘               │
                    │                       │
              同一件事的两面            RAG语义检索
                                            │
                                ┌───────────┼───────────┐
                                │           │           │
                           query向量   文档向量库     Top-K排序
                                │           │           │
                                └─────┬─────┘           │
                                      │                 │
                                 点积计算 ──────────→ 相似度排名
```

这张图的核心信息只有一个，点积的代数定义和几何意义是一枚硬币的两面，代数面让它计算高效，几何面让它具有度量相似性的能力。余弦相似度只是给点积做了一层归一化包装，而RAG检索就是在大规模地反复调用这个运算。

---

## 最后，我的一点个人感想

回顾整篇文章，我想说的是，点积这个概念之所以值得花这么多篇幅来讲，是因为它具有一种很罕见的品质，它足够简单到可以在硬件层面极致优化，同时又足够深刻到能承载语义相似性这样抽象的概念。

很多人在学AI的时候喜欢追新模型、新框架、新论文，这当然没有错。但我越来越觉得，真正让你建立深度理解的，往往是那些最基础的东西。点积就是一个完美的例子。你不需要理解Transformer的每一个细节，也不需要读过Attention的原始论文，但如果你透彻地理解了点积，你就能明白Attention机制里Q和K相乘在做什么，你就能理解向量数据库为什么能做语义检索，你就能看穿RAG系统的检索环节到底在干什么。

这种从一个简单概念出发，向外延伸理解整个系统的能力，我觉得是真正有价值的学习方式。

希望这篇文章能帮你把点积这个概念彻底吃透。下次当你用向量数据库做检索的时候，当你调试RAG系统的召回效果的时候，想想背后那个朴素的乘一乘加一加的操作，你会对整个系统有更深的理解和更强的掌控力。
