

# 向量课程总结，一次讲透文本变向量的底层逻辑

## 先聊聊为什么要学向量

很多同学一听到向量这个词，脑子里第一反应就是高中数学课本上那些带箭头的线段。方向、大小、坐标，考试的时候算来算去，毕业以后就再也没碰过了。但如果你现在开始接触大模型、AI应用开发、RAG检索增强生成这些领域，你会发现向量这个概念几乎无处不在。什么向量数据库、文本向量化、向量检索，满屏都是它。

所以今天这篇文章，我想从最底层的角度，带大家重新理解向量这个东西。我们不搞数学推导，不写一堆公式，就用大白话把几个核心问题讲清楚。这几个问题分别是，文本为什么可以变成向量，为什么语义相近的文本在向量空间里距离也近，以及我们经常看到的768维向量到底意味着什么。

把这三个问题想通了，你对整个Embedding技术的理解会上一个大台阶。

## 向量到底是个什么东西

在正式讲文本向量之前，我们先花一分钟回顾一下向量的本质。

最简单的理解，向量就是一组有序的数字。比如 [3, 5] 就是一个二维向量，[1, 2, 3] 就是一个三维向量。高中的时候我们在平面直角坐标系里画箭头，那个箭头的终点坐标就是向量的表示。

但是请注意一个很关键的事情，向量的维度可以远远超过三维。我们人类能直观感受的空间最多就是三维，可是数学上四维、五维、一百维、七百六十八维，都是合法的。你没办法在脑子里想象一个768维的空间长什么样，但这完全不影响我们用数学去计算它。

向量最强大的地方就在于，它可以用来描述事物的特征。举一个特别接地气的例子，假设我们要描述一杯奶茶，可以用这几个维度，甜度、温度、奶味浓度、茶味浓度。那一杯热的、很甜的、奶味重的奶茶，就可以表示成 [0.9, 0.8, 0.85, 0.3] 这样一个四维向量。每一个数字对应一个特征维度，数值大小代表这个特征的强弱程度。

理解了这一层，后面的内容就好懂了。

## 文本为什么可以变成向量

这是很多初学者的第一个疑问。数字天然就是数字，图片可以拆成像素值也算是数字，可文本呢？文字是人类发明的符号系统，一个汉字、一个单词，它本身就是一个抽象的符号，怎么就能变成一串数字了？

要回答这个问题，我们得先搞清楚一件事，我们把文本变成向量的目的是什么。目的就是捕捉文本的语义信息，然后用数学的方式去处理它。

最早期的做法非常粗暴，叫做One-Hot编码。假设我们的词典里总共有5个词，猫、狗、鱼、鸟、兔，那猫就表示成 [1,0,0,0,0]，狗就是 [0,1,0,0,0]，以此类推。每个词用一个和词典一样长的向量来表示，只有自己对应的位置是1，其余全是0。

这种做法有一个致命的问题，它完全没有语义信息。在这种表示下，猫和狗的距离，跟猫和鱼的距离是一样的。但我们人类都知道，猫和狗都是常见的宠物，它们的语义关系应该比猫和鱼更近才对。

后来人们就开始想办法，能不能让向量本身就携带语义信息？这就有了Word2Vec、GloVe这些经典方法，再到后来的BERT、GPT这些大模型，它们做的事情本质上是一样的，就是通过大量的文本数据，让模型自己去学习每个词、每句话应该对应什么样的向量。

这个学习过程可以这样理解。模型读了海量的文本之后，它发现猫和狗经常出现在相似的上下文里，比如它们都会出现在宠物、喂养、可爱这些词的附近。模型就会自动把猫和狗的向量调整到比较近的位置。而猫和经济这两个词，几乎不会出现在相似的语境中，所以它们的向量就会离得很远。

说到这里，关键概念就出来了，向量的每一个维度，代表的是某种语义特征。

这个语义特征不像我们前面说的奶茶例子那么直观。你没办法指着768维向量里的第37个数字说，这个代表情感倾向，那个代表时态信息。这些维度的含义是模型在训练过程中自己学出来的，是一种分布式的表征。每一个维度可能混合了多种语义信号，而某种语义信息也可能分散在好几个维度上。

但整体来看，这些维度合在一起，就构成了对一段文本语义的完整刻画。就好比一个人的性格，你很难用一个词来概括，但你可以用外向程度、严谨程度、亲和力、情绪稳定性、开放性这五个维度来描述，这就是心理学上的大五人格模型。文本向量做的事情也类似，只不过它用的维度多得多，精细得多。

下面用一段代码来直观感受一下文本变成向量的过程。

```python
from sentence_transformers import SentenceTransformer

# 加载一个预训练的Embedding模型
model = SentenceTransformer('all-MiniLM-L6-v2')

# 准备几段文本
sentences = [
    "今天天气真好，适合出去散步",
    "外面阳光明媚，想出门走走",
    "股票市场今天大跌，投资者情绪低迷"
]

# 将文本转换为向量
embeddings = model.encode(sentences)

# 查看第一个句子的向量
print(f"向量维度, {embeddings[0].shape}")
print(f"前10个数值, {embeddings[0][:10]}")
```

运行这段代码，你会看到每个句子都变成了一个384维的向量（这个模型输出384维），每个维度上都是一个浮点数。这一串数字，就是这句话在语义空间中的坐标。

## 为什么语义相近的文本向量距离也近

理解了文本可以变成向量之后，第二个核心问题就自然浮出来了。为什么语义相近的两句话，它们的向量在高维空间中会靠得很近？

这个问题其实在上一节已经埋下了伏笔。模型在训练的时候，目标函数就是这么设计的。

我们拿BERT举例。BERT在预训练的时候有一个任务叫做遮罩语言模型（MLM），它会随机遮住句子中的某些词，然后让模型根据上下文去预测被遮住的词是什么。这个过程迫使模型去理解上下文的语义关系。当模型能够准确预测被遮住的词时，它内部形成的那些向量表示，就已经蕴含了丰富的语义信息。

到了Sentence-BERT这类专门做句子级别Embedding的模型，训练策略就更直接了。它会拿一批语义相近的句子对作为正样本，语义不相关的句子对作为负样本，然后告诉模型，你要让正样本对的向量距离尽可能小，负样本对的向量距离尽可能大。

你看，模型就是被这样训练出来的。语义相近的文本向量距离近，这件事情本身就是训练目标。模型从海量数据中学习，不断调整内部参数，最终达到了这个效果。

用一个比喻来说明。想象一个巨大的图书馆，最开始所有的书都是随机堆放的。然后来了一个超级图书管理员，他把所有的书都读了一遍，根据内容的相似程度重新摆放。讲烹饪的书放在一起，讲历史的书放在一起，讲物理的和讲化学的靠得近一些但有一定距离。经过漫长的整理之后，你随手拿起一本书，它旁边的书大概率和它讲的是相关的内容。

Embedding模型做的就是这个图书管理员的工作，只不过它整理的空间是768维的，而且处理的是数以亿计的文本片段。

我们可以通过计算余弦相似度来验证这一点。

```python
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# 接上面的代码，计算三个句子之间的相似度
similarity_matrix = cosine_similarity(embeddings)

print("相似度矩阵")
for i, s1 in enumerate(sentences):
    for j, s2 in enumerate(sentences):
        print(f"句子{i+1} vs 句子{j+1}, {similarity_matrix[i][j]:.4f}")
```

运行结果你会发现，今天天气真好适合出去散步和外面阳光明媚想出门走走这两句话的相似度会很高，可能在0.8以上。而它们跟股票市场今天大跌那句话的相似度就会低很多，可能只有0.1左右。

这就是向量的威力。人类判断两句话意思是否相近，靠的是理解；机器判断两句话意思是否相近，靠的是计算向量之间的距离。结果殊途同归。

在这里多说一句关于距离度量的事情。常见的度量方式有三种，余弦相似度、欧氏距离和点积。余弦相似度衡量的是两个向量方向上的一致性，不管长度；欧氏距离就是我们直觉上理解的空间中两点之间的直线距离；点积则同时考虑了方向和长度。实际项目中用得最多的是余弦相似度，因为它对向量的缩放不敏感，更适合语义比较的场景。

```
常见向量距离度量方式对比

度量方式        计算方式              取值范围        适用场景
余弦相似度      方向夹角的余弦值      [-1, 1]        语义相似度比较
欧氏距离        空间中的直线距离      [0, +∞)        聚类分析
点积            对应维度相乘求和      (-∞, +∞)       推荐系统排序
```

## 768维向量到底意味着什么

终于说到这个话题了。我们在各种技术文档里经常看到，某某模型的输出维度是768，某某模型是1024，某某模型是384。这个数字到底代表什么？

简单来说，768维意味着模型用768个不同的语义特征维度来描述一段文本。

我们回到前面奶茶的例子。如果我只用甜度和温度两个维度来描述奶茶，那同样是热的甜奶茶，不管是用茉莉绿茶做底还是用红茶做底，在这个二维空间里看起来就差不多。但如果我加上茶底类型、珍珠含量、杯型大小等更多维度，就能把不同的奶茶区分得更精细。

文本向量也是同样的道理。维度越多，模型能捕捉到的语义细节就越丰富。768维相当于模型有768个语义探测器，每一个探测器负责感知文本中某一方面的语义信号。有的探测器可能对情感极性比较敏感，有的可能对专业领域比较敏感，有的可能对句式结构比较敏感。它们共同协作，最终给出一个全方位的语义画像。

为什么很多模型选择768这个数字呢？这跟BERT的架构设计有关。BERT-base的隐藏层维度就是768，后来很多模型都沿用了这个设定。当然也有模型用其他维度，比如all-MiniLM-L6-v2用的是384维，OpenAI的text-embedding-3-large可以支持到3072维。

那是不是维度越高越好呢？这个问题值得展开说说。

维度高的好处显而易见，语义表达能力更强，能区分更细微的语义差别。但代价也很明显。首先是存储成本，一个768维的float32向量占用3KB左右的空间，如果你有一千万条文本需要向量化，光存储向量就需要大约30GB。如果维度翻倍到1536，存储需求也翻倍。其次是计算成本，向量检索的时候需要计算距离，维度越高计算量越大，检索速度就越慢。

```
不同维度的Embedding模型对比

模型名称                      维度      存储/条      适用场景
all-MiniLM-L6-v2             384       1.5KB       轻量级应用,快速检索
BERT-base                    768       3KB         通用语义理解
text-embedding-3-large       3072      12KB        高精度语义匹配
```

所以实际选型的时候，你得根据自己的业务场景来权衡。如果你做的是一个内部知识库问答系统，文档量不大，对精度要求高，那选高维度的模型没问题。但如果你要处理海量数据，对响应速度有要求，那384维的轻量模型可能是更务实的选择。

另外还有一点需要注意，维度高并不能保证效果好。模型的训练数据质量、训练方法、模型架构这些因素的影响可能比单纯增加维度要大得多。一个在高质量中文语料上精心训练的768维模型，效果很可能吊打一个在混杂数据上粗糙训练的1536维模型。别迷信参数，要看实际效果。

## 从直觉到应用，向量思维带来了什么

学到这里，我们已经把三个核心问题都回答清楚了。但我还想再往前推一步，聊聊理解了向量之后，它在实际应用中意味着什么。

最直接的应用就是语义搜索。传统的关键词搜索是基于字面匹配的，你搜怎么缓解头痛，它会去找包含缓解和头痛这些关键词的文档。但如果一篇文章写的是偏头痛的治疗方法，虽然语义上完全相关，但因为没有出现缓解这个词，传统搜索可能就漏掉了。而基于向量的语义搜索就不一样了，它比较的是语义层面的相似度，即使用词完全不同，只要意思相近，就能被检索到。

```
语义搜索工作流程

用户输入查询 → 查询文本向量化 → 在向量数据库中检索相近向量 → 返回对应的原始文本 → 展示结果
```

第二个重要应用是RAG，也就是检索增强生成。现在大语言模型最大的问题之一就是幻觉，它会一本正经地编造不存在的信息。RAG的思路是在模型回答问题之前，先从知识库中检索相关文档片段，把这些真实的参考资料喂给模型，让模型基于这些资料来生成回答。而这个检索的过程，核心就是向量相似度计算。

第三个应用是文本聚类和分类。当你有大量的客服对话记录，想要自动分析用户都在抱怨什么问题的时候，你可以把每条记录都转成向量，然后用聚类算法在向量空间中把相似的记录归到一起。每一个簇就代表一类问题。这比人工一条条去看高效太多了。

还有推荐系统、去重检测、情感分析等等，底层用的都是向量化这套技术。可以说，一旦你掌握了把非结构化数据转成向量的思维方式，很多之前觉得很难的问题都能找到清晰的解题路径。

## 一些容易踩的坑

在实际使用向量的过程中，有几个常见的坑我想提醒大家注意。

第一个坑是混用不同模型的向量。不同的Embedding模型生成的向量是在不同的语义空间里的，你用模型A生成了文档向量存进数据库，查询的时候却用模型B来生成查询向量，这样计算出来的相似度毫无意义。一定要确保入库和查询使用同一个模型。

第二个坑是忽略文本长度的影响。大部分Embedding模型对输入文本的长度有限制，比如512个token。如果你的文档很长，直接把整篇文章扔进去，超出的部分会被截断，语义信息就丢失了。正确的做法是先把长文档切分成合适的片段，每个片段单独向量化。

第三个坑是以为向量搜索可以解决一切问题。向量搜索擅长处理语义模糊匹配的场景，但对于精确匹配的需求，比如用户要搜一个特定的订单号、一个具体的产品型号，传统的关键词搜索反而更靠谱。实际项目中，最好的方案往往是混合检索，把关键词搜索和向量搜索结合起来使用。

## 总结一下

我们今天从三个角度拆解了向量这个概念。

文本可以变成向量，是因为模型通过海量文本训练，学会了用一组数字来表征文本的语义信息，每个维度对应某种语义特征。相似文本的向量距离近，本质上是因为模型的训练目标就是让语义相近的内容在高维空间中聚集在一起。768维向量意味着模型从768个不同的语义角度来刻画一段文本，维度越多理论上表达能力越强，但也要付出存储和计算的代价。

我觉得理解向量最重要的一个认知转变是，向量让语义变得可计算了。在向量出现之前，语义是模糊的、主观的、难以量化的东西。两句话意思像不像，只能靠人来判断。但有了向量之后，语义被映射到了一个数学空间里，像不像这个问题变成了一个距离计算问题。这是整个现代NLP和AI应用能够落地的基石之一。

如果你正在学习大模型相关的技术栈，向量绝对是你需要最先吃透的基础概念。它看起来简单，就是一堆数字，但背后的思想贯穿了从数据处理到模型推理的每一个环节。把这个地基打扎实了，后面学RAG、学Agent、学微调，都会顺畅很多。
