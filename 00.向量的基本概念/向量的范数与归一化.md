

# 向量的范数与归一化，搞懂这些你才算真正入门

## 为什么要聊范数这个话题

很多同学在刚接触机器学习或者深度学习的时候，会遇到一个看起来很学术的词，叫做范数。第一次看到这两个字，大部分人的反应是一脸懵，觉得这又是某个高深的数学概念，离自己很远。但说实话，范数这东西一点都不复杂，它本质上就是在回答一个特别朴素的问题，一个向量到底有多大。

你想想看，一个标量比如数字5，我们很容易说它的大小就是5。但是如果给你一个向量，比如 [3, 4]，你怎么衡量它的大小呢？你总不能说它的大小是3，也不能说是4，因为它是一个整体。这时候就需要一套规则来计算向量的长度或者说大小了，这套规则就是范数。

范数的英文叫 norm，你可以把它理解为一种度量工具。就好像尺子是用来量长度的，秤是用来量重量的，范数就是用来量向量大小的。不同的范数用不同的方式来衡量，得到的结果也不一样。今天我们重点聊两种最常用的范数，L1范数和L2范数，然后再讲讲和它们密切相关的归一化操作。

## 先来看看什么是L1范数

L1范数有个很接地气的别名，叫曼哈顿距离。为什么叫这个名字呢？你想象一下纽约曼哈顿的街道，那里的街区是方方正正的网格状结构。如果你要从A点走到B点，你没办法斜着穿过楼房，你只能沿着街道横着走或者竖着走。这段路程的总长度，就是曼哈顿距离的含义。

落到数学上，L1范数的计算方式非常简单，就是把向量里每个元素的绝对值加起来。

比如有个向量 v = [3, -4, 2]，它的L1范数就是 |3| + |-4| + |2| = 3 + 4 + 2 = 9。

就这么简单，没有任何花哨的操作。你看到每个分量，不管它是正是负，先取绝对值，然后全部加在一起，得到的那个数就是这个向量的L1范数。在数学符号上，我们一般写成 ||v||₁ 来表示。

L1范数有个很有意思的特性，它对每个维度的贡献是线性累加的。什么意思呢？就是说每个维度的值对最终范数的影响是等比例的。一个分量从2变成4，范数就增加2，不会因为某种非线性的计算而产生不同的效果。这个特性让L1范数在某些应用场景下特别好用，后面我们会具体聊到。

这里有个小细节值得注意，L1范数只关心绝对值的总和，它并不关心这些值具体是怎么分布的。比如向量 [9, 0, 0] 和向量 [3, 3, 3]，它们的L1范数都是9，但显然这两个向量长得完全不一样。一个很集中，一个很分散。这就是L1范数的特点，它提供了一种衡量大小的方式，但不是唯一的方式。

## 再来认识L2范数

说完L1，我们来看L2范数。L2范数的名气可能比L1还大，因为它有个大家从小就认识的别名，叫欧几里得距离。没错，就是初中数学里学过的那个两点之间距离公式的推广版本。

L2范数的计算方式是把每个元素先平方，然后全部加起来，最后对总和开一个根号。

还是用刚才那个向量 v = [3, -4, 2] 来算，L2范数就是 √(3² + (-4)² + 2²) = √(9 + 16 + 4) = √29 ≈ 5.39。

如果你回想一下二维平面上的勾股定理，其实L2范数就是勾股定理在高维空间的自然推广。在二维平面上，向量 [3, 4] 的长度是 √(9+16) = 5，这和我们初中学的那个 3-4-5 直角三角形完全吻合。把这个思路扩展到三维、四维甚至几百维的空间，就是L2范数在做的事情。

L2范数和L1范数最大的区别在于平方操作的引入。因为要先平方，所以那些绝对值比较大的分量对最终结果的影响会被放大。比如向量 [1, 1, 1, 10]，L1范数是13，而L2范数是 √(1+1+1+100) ≈ 10.15。你能明显感觉到，L2范数更多地被那个值为10的分量所主导了。这个特性让L2范数对异常大的值更加敏感，在实际应用中这可能是好事也可能是坏事，取决于具体场景。

## L1和L2到底有什么区别，怎么选

到这里，可能有同学要问了，既然两种范数都是衡量向量大小的，那我到底用哪一个呢？这个问题确实很实际，我们来好好比较一下。

从几何直觉上看，L1范数度量的是走街道的距离，L2范数度量的是直线距离。同样是从原点到某个点，两种度量方式给出的数值通常不一样。

从对各个分量的敏感程度来看，L1范数对所有分量一视同仁，而L2范数因为平方的存在，会更偏向于关注那些值比较大的分量。这就导致了一个很重要的应用差异。

在机器学习中做正则化的时候，L1正则化（也叫Lasso）倾向于产生稀疏解，也就是说它会让模型参数中的很多权重直接变成0。这个特性使得L1正则化天然带有特征选择的功能，它会帮你挑出哪些特征是重要的，把不重要的直接砍掉。

而L2正则化（也叫Ridge）倾向于让所有权重都变小但不会变成0，它产生的结果更加平滑。当你的特征之间存在多重共线性的时候，L2正则化表现得往往比L1好。

所以选哪个，看你的需求。如果你希望模型简洁，特征少一点，用L1。如果你希望模型稳健，参数平滑一些，用L2。实际工程中，很多时候两个一起用，这就是所谓的弹性网络正则化。

## 接下来进入重头戏，向量归一化

理解了范数之后，归一化就顺理成章了。向量归一化的操作极其简单，就是把一个向量除以它的范数，让它变成一个长度为1的向量，也就是所谓的单位向量。

用L2范数做归一化的公式是这样的，对于向量 v，归一化后的向量 v̂ = v / ||v||₂。

举个具体例子，向量 v = [3, 4]，它的L2范数是5，归一化之后就是 [3/5, 4/5] = [0.6, 0.8]。你可以验证一下，0.6² + 0.8² = 0.36 + 0.64 = 1，确实是单位向量。

归一化之后的向量有一个非常重要的性质，它保留了原始向量的方向信息，但是丢掉了长度信息。换句话说，归一化只关心向量指向哪里，不关心它有多长。

你可能觉得丢掉长度信息是个损失，但在很多场景下这恰恰是我们想要的。打个比方，两篇文章分别是1000字和5000字，用词频向量表示之后，长文章的向量各分量自然会更大。但我们比较两篇文章的主题相似度时，不应该因为一篇更长就认为它们不相似。归一化之后，两篇文章都变成了单位向量，我们只比较方向，不受篇幅影响，这就很合理了。

这里有个细节需要注意，归一化操作要求范数不能为0。也就是说零向量是没办法归一化的，因为你没法除以0。在实际写代码的时候，要注意对这种边界情况做处理。

## 归一化在Embedding领域的核心应用

说到归一化最典型的应用场景，就不得不提embedding了。如果你接触过自然语言处理、推荐系统或者图像检索，你一定知道embedding这个概念。简单说，embedding就是把一个对象（一个词、一句话、一张图）用一个高维向量来表示。

在实际使用embedding的时候，我们通常会在得到向量之后立刻做一步L2归一化。为什么要这么做呢？原因有好几个。

第一个原因和相似度计算有关。我们衡量两个embedding之间相似度最常用的方法是余弦相似度，它的计算公式是两个向量的点积除以它们各自范数的乘积。如果两个向量都已经是单位向量了，那分母就是1，余弦相似度就直接等于点积。这极大地简化了计算，在需要做海量相似度比较的场景下（比如搜索引擎），这种简化带来的性能提升是非常可观的。

第二个原因和数值稳定性有关。不同模型、不同层产出的embedding，其数值范围可能差异很大。有的向量分量在0.001这个量级，有的在100这个量级。如果不做归一化就直接比较或者计算，很容易出现数值溢出或者精度损失的问题。归一化相当于把所有向量统一到同一个度量尺度上，计算起来更稳定。

第三个原因比较微妙，和向量数据库的索引效率有关。现在很多向量数据库比如Milvus、Pinecone、Faiss在建索引的时候，对归一化后的向量可以使用更高效的检索算法。因为所有向量都落在单位超球面上，搜索空间被有效约束了，检索速度自然更快。

我在实际项目中有个深刻体会，之前做一个语义搜索的系统，用的是某个预训练模型输出的sentence embedding。一开始没有做归一化，检索结果总是差强人意，有些明显语义相关的句子排名却不高。后来加了一步L2归一化，检索质量立刻有了明显的提升。后来分析原因发现，那个模型输出的向量在不同句子长度下，范数差异很大，短句子的范数普遍偏小，长句子的范数偏大。不归一化的话，长度信息会干扰语义相似度的判断。

## 用代码来实操一下

光说不练假把式，我们用Python代码把上面讲的内容都跑一遍。

```python
import numpy as np

# 定义一个向量
v = np.array([3, -4, 2])
print(f"原始向量, {v}")

# 计算L1范数
l1_norm = np.sum(np.abs(v))
# 也可以直接用numpy的内置函数
l1_norm_np = np.linalg.norm(v, ord=1)
print(f"L1范数（手动计算）, {l1_norm}")
print(f"L1范数（numpy函数）, {l1_norm_np}")

# 计算L2范数
l2_norm = np.sqrt(np.sum(v ** 2))
l2_norm_np = np.linalg.norm(v, ord=2)
print(f"L2范数（手动计算）, {l2_norm:.4f}")
print(f"L2范数（numpy函数）, {l2_norm_np:.4f}")

# L2归一化
v_normalized = v / l2_norm
print(f"归一化后的向量, {v_normalized}")
print(f"归一化后的范数, {np.linalg.norm(v_normalized):.4f}")

print("---")

# 验证归一化后余弦相似度等于点积
a = np.array([1, 2, 3, 4])
b = np.array([4, 3, 2, 1])

# 未归一化时计算余弦相似度
cos_sim = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
print(f"未归一化时的余弦相似度, {cos_sim:.4f}")

# 归一化后直接用点积
a_norm = a / np.linalg.norm(a)
b_norm = b / np.linalg.norm(b)
dot_product = np.dot(a_norm, b_norm)
print(f"归一化后的点积, {dot_product:.4f}")
print(f"两者是否相等, {np.isclose(cos_sim, dot_product)}")
```

运行结果会清楚地展示，归一化后向量的范数变成了1，而且归一化后两个向量的点积和原始的余弦相似度完全一致。

再来看一个更贴近实际应用的例子，模拟embedding归一化在语义搜索中的使用。

```python
import numpy as np

# 模拟5个文档的embedding（假设维度为8）
np.random.seed(42)
doc_embeddings = np.random.randn(5, 8)

# 人为给不同文档设置不同的范数大小，模拟真实场景
scales = [0.5, 2.0, 1.0, 5.0, 0.1]
for i in range(5):
    doc_embeddings[i] *= scales[i]

print("归一化前各文档的L2范数,")
for i, emb in enumerate(doc_embeddings):
    print(f"  文档{i}, {np.linalg.norm(emb):.4f}")

# L2归一化
doc_embeddings_normalized = doc_embeddings / np.linalg.norm(
    doc_embeddings, axis=1, keepdims=True
)

print("\n归一化后各文档的L2范数,")
for i, emb in enumerate(doc_embeddings_normalized):
    print(f"  文档{i}, {np.linalg.norm(emb):.4f}")

# 模拟一个查询向量
query = np.random.randn(8)
query_normalized = query / np.linalg.norm(query)

# 用点积计算相似度（归一化后等价于余弦相似度）
similarities = doc_embeddings_normalized @ query_normalized
print("\n各文档与查询的相似度,")
for i, sim in enumerate(similarities):
    print(f"  文档{i}, {sim:.4f}")

# 找出最相似的文档
best_match = np.argmax(similarities)
print(f"\n最相似的文档是, 文档{best_match}")
```

这段代码模拟了一个小型语义搜索的完整流程。你可以看到，归一化之前不同文档的范数差异很大，从0.几到十几都有。归一化之后都变成了1.0，这样用点积计算相似度就非常干净利落了。

## 用思维导图梳理一下整体知识脉络

```
向量的范数与归一化
├── 范数的本质
│   └── 衡量向量的大小/长度
│
├── L1范数（曼哈顿距离）
│   ├── 计算方式, 绝对值之和
│   ├── 几何含义, 沿坐标轴走的总距离
│   ├── 特点, 对各分量线性累加
│   └── 应用, L1正则化（Lasso）产生稀疏解
│
├── L2范数（欧几里得距离）
│   ├── 计算方式, 平方和开根号
│   ├── 几何含义, 直线距离（勾股定理推广）
│   ├── 特点, 对大值分量更敏感
│   └── 应用, L2正则化（Ridge）产生平滑解
│
├── 向量归一化
│   ├── 操作, 向量除以其范数
│   ├── 结果, 得到长度为1的单位向量
│   ├── 保留方向信息, 丢弃长度信息
│   └── 注意, 零向量不能归一化
│
└── 应用场景
    ├── Embedding归一化
    │   ├── 简化余弦相似度计算（点积即可）
    │   ├── 提升数值稳定性
    │   └── 提高向量数据库检索效率
    ├── 机器学习正则化
    └── 文本/图像相似度检索
```

## 再聊几个容易踩的坑

在实际使用范数和归一化的过程中，有几个常见的误区我觉得值得单独拿出来说说。

第一个坑是混淆向量范数和矩阵范数。虽然都叫范数，但矩阵范数的定义和计算方式跟向量范数有很大不同。比如矩阵的Frobenius范数虽然看起来和L2范数很像，但它作用的对象是矩阵里的所有元素。如果你在写代码的时候不小心把矩阵传进了计算向量范数的函数里，可能不会报错但结果完全不是你想要的。

第二个坑是在做batch归一化的时候搞错了维度。在用numpy或者pytorch处理一批向量的时候，你需要沿着正确的axis来计算范数。如果你有一个shape为 (batch_size, embedding_dim) 的矩阵，你应该沿着axis=1来算范数，这样每一行（每个向量）都除以自己的范数。搞错了axis，你就会沿着batch那个维度做归一化，结果完全就乱了。

第三个坑是忘记处理零向量的情况。虽然在大多数情况下embedding不太会是全零向量，但在某些极端情况下确实可能出现。比如某些padding操作或者模型对未知输入的默认输出。遇到零向量直接除以0，程序要么报错要么输出nan，然后nan会像病毒一样传染到后续所有的计算中。加一个极小值eps（比如1e-8）到分母上，就能有效避免这个问题。

```python
# 安全的归一化写法
def safe_normalize(v, eps=1e-8):
    norm = np.linalg.norm(v, axis=-1, keepdims=True)
    return v / (norm + eps)
```

## 总结一下

回顾整篇文章的内容，我们从一个最基本的问题出发，怎么衡量一个向量的大小。L1范数用绝对值求和的方式给出了一个答案，L2范数用平方和开根号给出了另一个答案。两种范数各有各的几何含义和应用场景，并不存在哪个绝对更好的说法。

在范数的基础上，归一化操作把向量压缩到单位长度，只保留方向信息。这个看似简单的操作在embedding的世界里却是至关重要的一步，它让相似度计算变得更简洁，让数值更稳定，让检索更高效。

说到底，范数和归一化都是工具。理解了它们背后的数学原理和直觉含义，你才能在实际工作中知道什么时候该用、怎么用、为什么用。希望这篇文章能帮你把这些基础概念真正搞明白，而不只是停留在知道有这么个东西的层面上。数学基础扎实了，后面学更复杂的东西才会事半功倍。
